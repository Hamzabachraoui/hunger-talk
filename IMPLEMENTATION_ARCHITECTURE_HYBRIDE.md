# üìã Impl√©mentation Architecture Hybride

## üéØ Objectif

**Architecture hybride** :
- Railway : Backend (auth, stock, recettes, etc.)
- Ollama Local : Appel√© directement depuis l'app mobile (via WiFi local)

## üìù Plan d'Impl√©mentation

### 1. Service Ollama dans l'App Mobile

Cr√©er `mobile/lib/data/services/ollama_service.dart` :
- R√©cup√®re l'IP Ollama depuis Railway (`/api/system-config/ollama`)
- Appelle Ollama directement via HTTP
- G√®re les erreurs de connexion

### 2. Modification du ChatService

Modifier `mobile/lib/data/services/chat_service.dart` :
- Option A : Appeler Ollama directement (simple, mais perd le contexte RAG)
- Option B : R√©cup√©rer le contexte depuis Railway, puis appeler Ollama (meilleur)

### 3. Option Recommand√©e : Hybride avec Contexte

Pour garder le contexte RAG (stock, recettes, etc.) :

1. **L'app appelle Railway pour obtenir le contexte** :
   ```
   POST /api/chat/context
   ‚Üí Retourne : {"context": "Stock: lait, oeufs... Recettes: ..."}
   ```

2. **L'app appelle Ollama directement avec le contexte** :
   ```
   POST http://192.168.11.101:11434/api/chat
   ‚Üí Message + Contexte
   ```

3. **Optionnel : L'app sauvegarde dans Railway** :
   ```
   POST /api/chat/save
   ‚Üí Sauvegarde le message et la r√©ponse
   ```

### 4. Option Simple : Appel Direct (Sans Contexte)

Si vous voulez juste un chat simple sans contexte RAG :

1. L'app r√©cup√®re l'IP Ollama depuis Railway
2. L'app appelle Ollama directement
3. Pas de contexte (pas de stock, recettes, etc.)

**Avantage** : Plus simple, plus rapide
**Inconv√©nient** : Perd le contexte (pas de recommandations bas√©es sur le stock)

## üîÑ Flux Recommand√© (Avec Contexte)

```
1. Utilisateur envoie un message
   ‚Üì
2. App appelle Railway : POST /api/chat/context
   ‚Üí Railway construit le contexte RAG (stock, recettes, etc.)
   ‚Üí Retourne le contexte
   ‚Üì
3. App r√©cup√®re l'IP Ollama depuis Railway (cach√©e)
   ‚Üì
4. App appelle Ollama directement : POST http://IP:11434/api/chat
   ‚Üí Message + Contexte
   ‚Üí Ollama r√©pond
   ‚Üì
5. Optionnel : App sauvegarde dans Railway : POST /api/chat/save
```

## üìù Modifications N√©cessaires

### Backend Railway

Cr√©er un endpoint pour obtenir juste le contexte :
```python
@router.post("/chat/context")
async def get_chat_context(
    chat_data: ChatMessageCreate,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Retourne le contexte RAG sans appeler Ollama"""
    rag_service = RAGService(db, current_user)
    context = rag_service.build_full_context(chat_data.message)
    system_prompt = rag_service.build_system_prompt()
    return {
        "context": context,
        "system_prompt": system_prompt
    }
```

### App Mobile

1. Cr√©er `OllamaService` (d√©j√† fait)
2. Modifier `ChatService` pour utiliser `OllamaService`
3. R√©cup√©rer le contexte depuis Railway avant d'appeler Ollama

---

**Recommandation** : Option Hybride avec Contexte pour garder toutes les fonctionnalit√©s RAG !

