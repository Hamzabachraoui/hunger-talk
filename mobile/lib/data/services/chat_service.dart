import 'package:flutter/foundation.dart';
import 'api_service.dart';
import 'ollama_service.dart';
import '../models/chat_message_model.dart';
import '../../core/constants/app_constants.dart';

class ChatService {
  final ApiService _apiService = ApiService();
  final OllamaService _ollamaService = OllamaService();

  /// Envoie un message via Ollama local (architecture hybride)
  /// 
  /// 1. R√©cup√®re le contexte RAG depuis Railway
  /// 2. Appelle Ollama localement avec le contexte et streaming
  /// 3. Retourne la r√©ponse de l'IA
  /// [onChunk] est appel√© √† chaque chunk re√ßu pour mettre √† jour l'UI en temps r√©el
  Future<String> sendMessage(String message, {Function(String)? onChunk}) async {
    try {
      debugPrint('üí¨ [CHAT] Envoi de message: ${message.substring(0, message.length > 50 ? 50 : message.length)}...');
      
      // 1. R√©cup√©rer le contexte RAG depuis Railway
      debugPrint('üîç [CHAT] R√©cup√©ration du contexte depuis Railway...');
      final contextData = {'message': message};
      final contextResponse = await _apiService.post(
        '/chat/context',
        contextData,
        timeout: AppConstants.apiTimeout,
      );
      
      String? context;
      String? systemPrompt;
      
      if (contextResponse != null && contextResponse is Map<String, dynamic>) {
        context = contextResponse['context'] as String?;
        systemPrompt = contextResponse['system_prompt'] as String?;
        debugPrint('‚úÖ [CHAT] Contexte r√©cup√©r√© depuis Railway (${context?.length ?? 0} caract√®res)');
      } else {
        debugPrint('‚ö†Ô∏è [CHAT] Contexte non disponible, envoi sans contexte');
      }
      
      // 2. Appeler Ollama localement avec le contexte et streaming
      debugPrint('ü§ñ [CHAT] Appel √† Ollama local avec streaming...');
      final aiResponse = await _ollamaService.sendMessage(
        message,
        context: context,
        systemPrompt: systemPrompt,
        onChunk: onChunk, // Passer le callback pour les mises √† jour progressives
      );
      
      debugPrint('‚úÖ [CHAT] R√©ponse IA re√ßue (${aiResponse.length} caract√®res)');
      return aiResponse;
      
    } catch (e) {
      debugPrint('‚ùå [CHAT] Erreur lors de l\'envoi: $e');
      rethrow;
    }
  }

  Future<List<ChatMessageModel>> getHistory() async {
    debugPrint('üí¨ [CHAT] Chargement de l\'historique...');
    final response = await _apiService.get(AppConstants.chatHistory);
    if (response == null) {
      debugPrint('‚ö†Ô∏è [CHAT] Historique vide');
      return [];
    }
    if (response is! List) {
      debugPrint('‚ùå [CHAT] Format de r√©ponse invalide: ${response.runtimeType}');
      throw Exception('Format de r√©ponse invalide: attendu List, re√ßu ${response.runtimeType}');
    }
    final List<dynamic> data = response;
    debugPrint('üí¨ [CHAT] ${data.length} message(s) dans l\'historique');
    return data.map((json) {
      if (json is! Map<String, dynamic>) {
        debugPrint('‚ùå [CHAT] Format d\'√©l√©ment invalide: ${json.runtimeType}');
        throw Exception('Format d\'√©l√©ment invalide: attendu Map, re√ßu ${json.runtimeType}');
      }
      return ChatMessageModel.fromJson(json);
    }).toList();
  }
}

